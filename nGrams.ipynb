{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6d102667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # gensim depends on numpy\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef219a78",
   "metadata": {},
   "source": [
    "# The Recovery Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4b95bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract the rows with is_recovery = 1\n",
    "sample = pd.read_csv('recovery_samples_discosure_subs - recovery_samples.csv')\n",
    "selected_columns = sample[['text', 'is_recovery']]\n",
    "df_rec = selected_columns.copy()\n",
    "df_rec.dropna(subset = ['is_recovery'], inplace = True)\n",
    "df_rec.dropna(subset = ['text'], inplace = True)\n",
    "df_rec.drop(columns = ['is_recovery'], inplace = True)\n",
    "df_rec.reset_index(drop = True, inplace = True) \n",
    "# reset the index, if not, there will be a gap caused by dropping na values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0777621d",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9d0abb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Define patterns to remove punctuations, numbers, new lines, multiple spaces\n",
    "pattern_punc = \"[^\\w\\s]\"\n",
    "pattern_num = \"[0-9]\"\n",
    "pattern_newline = \"\\n\"\n",
    "pattern_mulspace = \"\\s+\"\n",
    "pattern_longline = \"___\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "366f2f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Remove punctuation patterns\n",
    "for i in range(len(df_rec)):\n",
    "        # lowercase\n",
    "        df_rec['text'][i] = df_rec['text'][i].lower()\n",
    "        # remove puncuations\n",
    "        df_rec['text'][i] = re.sub(pattern_punc, '', df_rec['text'][i])\n",
    "        # remove numbers\n",
    "        df_rec['text'][i] = re.sub(pattern_num, '', df_rec['text'][i])\n",
    "        # remove new line (\\n)\n",
    "        df_rec['text'][i] = re.sub(pattern_newline, ' ', df_rec['text'][i])\n",
    "        # remove multiple space\n",
    "        df_rec['text'][i] = re.sub(pattern_mulspace, ' ', df_rec['text'][i])  \n",
    "        # remove long line\n",
    "        df_rec['text'][i] = re.sub(pattern_longline, ' ', df_rec['text'][i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f9b152e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Tokenization, put all rows into a list\n",
    "text_list = df_rec['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c1ae158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Tokenization using the split function \n",
    "word_list = []\n",
    "for i in text_list:\n",
    "    if type(i) != str:\n",
    "        continue\n",
    "    else:\n",
    "        words = i.split()\n",
    "        # word_list.append(words)\n",
    "        for word in words:\n",
    "            word_list.append(word)\n",
    "# Method 2: nltk.word_tokenize('here is your sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "02f12ed9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19760"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2e0036e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Edit the stop words list\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# Add some stop words \n",
    "add_stop = ['im', 'one', 'also', 'ive', 'etc', 'hi']\n",
    "stop_words_re = stop_words + add_stop\n",
    "# remove punctuations from stop words\n",
    "for i in range(len(stop_words_re)):\n",
    "    stop_words_re[i] = re.sub(pattern_punc, '', stop_words_re[i])\n",
    "    word_list_nostop = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aa212a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Remove stop words\n",
    "for word in word_list:\n",
    "    if word not in stop_words_re:\n",
    "        word_list_nostop.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "12c889e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = word_list_nostop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7ec09246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('q', 95), ('like', 93), ('people', 83), ('believe', 53), ('would', 46), ('know', 45), ('even', 45), ('time', 44), ('really', 43), ('conspiracy', 43), ('trump', 43), ('world', 42), ('think', 41), ('going', 41), ('qanon', 38), ('right', 35), ('much', 35), ('things', 34), ('want', 33), ('never', 32), ('back', 32), ('lot', 29), ('could', 29), ('way', 27), ('still', 27), ('started', 27), ('covid', 27), ('get', 26), ('everything', 26), ('help', 26), ('say', 25), ('feel', 25), ('true', 24), ('something', 24), ('reality', 24), ('us', 23), ('got', 23), ('family', 23), ('new', 22), ('stuff', 22), ('well', 22), ('government', 22), ('actually', 21), ('made', 21), ('believed', 21), ('story', 21), ('far', 21), ('thing', 20), ('qs', 20), ('years', 20)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Extract top 50 frequent unigram of the recovery group\n",
    "from collections import Counter\n",
    "Counter = Counter(unigram)\n",
    "most_common_unigram= Counter.most_common(50)\n",
    "print(most_common_unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7fee49",
   "metadata": {},
   "source": [
    "##  Bigram & Trigram (w/ stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9b769b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "58344ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate a list with no punctuation, but have stop words\n",
    "## The reason that keeps stop words for now is that sequence of words matter to express meaning\n",
    "text_split = []\n",
    "for i in range(len(text_list)):\n",
    "    text_split.append(text_list[i].split())\n",
    "# text_split is a list of list of string tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "56f4327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a dictionary\n",
    "dct = corpora.Dictionary(text_split)\n",
    "corpus = [dct.doc2bow(line) for line in text_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2a212543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build the bigram and trigram model\n",
    "bigram = gensim.models.phrases.Phrases(text_split, min_count=3, threshold=10)\n",
    "trigram = gensim.models.phrases.Phrases(bigram[text_split], threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "60458b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how the bigram looks \n",
    "# print(bigram[text_split[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d7e45ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Generate a flat list of unigram\n",
    "# Define a function to flatten the lists within list\n",
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist] \n",
    "# put unigrams in a flat list \n",
    "flat_uni = flatten(text_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6813c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 put bigram in a flat list\n",
    "bigrams_list = []\n",
    "for i in range(len(text_split)):\n",
    "    bigrams_list.append(bigram[text_split[i]])\n",
    "flat_uni_bi = flatten(bigrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b6493a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 create a list that only contains bigram, no unigram\n",
    "flat_bi = []\n",
    "flat_bi = [item for item in flat_uni_bi if item not in flat_uni]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "30d3a318",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to_be', 37), ('i_had', 32), ('i_dont', 30), ('a_lot', 29), ('i_am', 27), ('going_to', 21), ('want_to', 20), ('have_been', 16), ('the_whole', 14), ('a_few', 14), ('im_not', 13), ('to_say', 13), ('part_of', 12), ('they_are', 12), ('the_same', 12), ('as_well', 11), ('deep_state', 11), ('ive_been', 11), ('there_are', 11), ('conspiracy_theories', 10), ('my_story', 10), ('could_be', 10), ('who_are', 10), ('my_life', 10), ('thank_you', 10), ('kind_of', 10), ('people_who', 9), ('rabbit_hole', 9), ('i_realized', 9), ('if_you', 9), ('wanted_to', 9), ('in_order', 9), ('going_on', 9), ('looking_for', 9), ('my_family', 9), ('used_to', 8), ('as_much', 8), ('would_be', 8), ('my_mind', 8), ('people_like', 8), ('my_parents', 8), ('will_be', 8), ('know_what', 8), ('dont_know', 8), ('feel_like', 7), ('sort_of', 7), ('a_year', 7), ('thanks_to', 7), ('made_me', 7), ('my_mom', 7)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Extract top 50 frequent bigram of the recovery group\n",
    "from collections import Counter\n",
    "Counter = Counter(flat_bi)\n",
    "most_common_bigram = Counter.most_common(50)\n",
    "print(most_common_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6209b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c19c71fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put trigram in a flat list\n",
    "trigrams_list = []\n",
    "for i in range(len(text_split)):\n",
    "    trigrams_list.append(trigram[bigram[text_split[i]]])\n",
    "flat_uni_bi_tri = flatten(trigrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ea9c27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a list that only contains trigrams, no bigram, no unigram\n",
    "flat_tri = []\n",
    "flat_tri_temp = []\n",
    "flat_tri_temp = [item for item in flat_uni_bi_tri if item not in flat_bi]\n",
    "flat_tri = [item for item in flat_tri_temp if item not in flat_uni]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "04d59ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flat_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8f54bb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i_was', 67), ('but_i', 41), ('at_the', 27), ('it_was', 26), ('the_world', 25), ('as_a', 21), ('it_is', 21), ('a_lot_of', 21), ('out_of', 20), ('i_just', 17), ('i_think', 17), ('some_of', 15), ('back_to', 14), ('the_most', 13), ('about_it', 13), ('the_government', 11), ('for_me', 10), ('of_these', 10), ('the_deep_state', 9), ('the_election', 9), ('he_was', 8), ('to_take', 8), ('i_cant', 8), ('which_is', 8), ('in_order_to', 7), ('as_much_as', 6), ('i_did', 6)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Extract top 50 frequent trigram of the recovery group\n",
    "from collections import Counter\n",
    "Counter = Counter(flat_tri)\n",
    "most_common_trigram = Counter.most_common(50)\n",
    "print(most_common_trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b4a0c",
   "metadata": {},
   "source": [
    "# The Non-recovery Group w/ stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "291c31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract the rows with is_recovery being NaN\n",
    "df_non = selected_columns.copy()\n",
    "df_non.drop(df_non.index[df_non['is_recovery'] == 1.0], inplace=True)\n",
    "\n",
    "df_non.drop(columns = ['is_recovery'], inplace = True)\n",
    "df_non.dropna(subset = ['text'], inplace = True)\n",
    "\n",
    "df_non.reset_index(drop = True, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "009cc301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 1)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "eb2a9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Define patterns to remove punctuations, numbers, new lines, multiple spaces\n",
    "pattern_punc = \"[^\\w\\s]\"\n",
    "pattern_num = \"[0-9]\"\n",
    "pattern_newline = \"\\n\"\n",
    "pattern_mulspace = \"\\s+\"\n",
    "pattern_longline = \"___\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e9d6e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Remove punctuation patterns\n",
    "for i in range(len(df_non)):\n",
    "        # remove numbers\n",
    "        df_non['text'][i] = re.sub(pattern_num, '', df_non['text'][i])\n",
    "        # lowercase\n",
    "        df_non['text'][i] = df_non['text'][i].lower()\n",
    "        # remove puncuations\n",
    "        df_non['text'][i] = re.sub(pattern_punc, '', df_non['text'][i])\n",
    "        # remove new line (\\n)\n",
    "        df_non['text'][i] = re.sub(pattern_newline, ' ', df_non['text'][i])\n",
    "        # remove multiple space\n",
    "        df_non['text'][i] = re.sub(pattern_mulspace, ' ', df_non['text'][i])  \n",
    "        # remove long line\n",
    "        df_non['text'][i] = re.sub(pattern_longline, ' ', df_non['text'][i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fcb1f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Tokenization, put all rows into a list\n",
    "text_list_non = df_non['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "db6856db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Tokenization using the split function \n",
    "word_list_non = []\n",
    "for i in text_list_non:\n",
    "    if type(i) != str:\n",
    "        continue\n",
    "    else:\n",
    "        words = i.split()\n",
    "        # word_list.append(words)\n",
    "        for word in words:\n",
    "            word_list_non.append(word)\n",
    "# Method 2: nltk.word_tokenize('here is your sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "13507e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25223"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "33ab18d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Remove stop words\n",
    "word_list_non_nostop = []\n",
    "for word in word_list_non:\n",
    "    if word not in stop_words_re:\n",
    "        word_list_non_nostop.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3b829ad8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12138"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list_non_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "beaca769",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_non = word_list_non_nostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f24ceaf1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('qanon', 149), ('people', 107), ('like', 107), ('q', 88), ('would', 86), ('know', 82), ('please', 77), ('want', 61), ('share', 59), ('get', 56), ('even', 56), ('anything', 55), ('time', 52), ('conspiracy', 50), ('someone', 47), ('use', 46), ('news', 46), ('follow', 46), ('thread', 45), ('cause', 44), ('feel', 43), ('help', 43), ('videos', 43), ('remember', 43), ('keep', 41), ('really', 40), ('interesting', 40), ('related', 40), ('believe', 38), ('pictures', 38), ('links', 38), ('rules', 38), ('conversations', 38), ('civil', 38), ('anyone', 37), ('still', 36), ('podcasts', 36), ('research', 34), ('make', 33), ('first', 33), ('say', 33), ('back', 33), ('think', 33), ('trump', 32), ('going', 32), ('always', 31), ('could', 31), ('theories', 30), ('much', 30), ('see', 30)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Extract top 50 frequent unigram of the non-recovery group\n",
    "from collections import Counter\n",
    "Counter = Counter(unigram_non)\n",
    "most_common_unigram_non= Counter.most_common(50)\n",
    "print(most_common_unigram_non)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02697ed",
   "metadata": {},
   "source": [
    "## Bigram & Trigram - non-recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3b84a695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_list_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1339821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate a list with no punctuation, but have stop words\n",
    "## The reason that keeps stop words for now is that sequence of words matter to express meaning\n",
    "text_split_non = []\n",
    "for i in range(len(text_list_non)):\n",
    "    text_split_non.append(text_list_non[i].split())\n",
    "# text_split_non is a list of list of string tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8041f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a dictionary\n",
    "dct_non = corpora.Dictionary(text_split_non)\n",
    "corpus_non = [dct_non.doc2bow(line) for line in text_split_non]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2a5bef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build the bigram and trigram model\n",
    "bigram_non = gensim.models.phrases.Phrases(text_split_non, min_count=3, threshold=5)\n",
    "trigram_non = gensim.models.phrases.Phrases(bigram_non[text_split_non], threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "daae0e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i_have', 'created', 'this', 'subreddit', 'for_those', 'who_have', 'escaped', 'qanon', 'feel_free', 'to_share', 'your', 'stories', 'and', 'your', 'struggles', 'how_did', 'you', 'get', 'into_q', 'how_did', 'you', 'get', 'out', 'this_is', 'the', 'sistersubreddit', 'of', 'rqanoncasualties', 'i_hope', 'this', 'community', 'will_be', 'as', 'helpful', 'to', 'you', 'as', 'that', 'community', 'has_been', 'for_those', 'who_have', 'lost', 'family', 'and', 'friends', 'to', 'q']\n"
     ]
    }
   ],
   "source": [
    "# see how the bigram looks \n",
    "print(bigram_non[text_split_non[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "45b4d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Generate a flat list of unigram\n",
    "# put unigrams in a flat list \n",
    "flat_uni_non = flatten(text_split_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "43d8993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 put bigram in a flat list\n",
    "bigrams_list_non = []\n",
    "for i in range(len(text_split_non)):\n",
    "    bigrams_list_non.append(bigram_non[text_split_non[i]])\n",
    "flat_uni_bi_non = flatten(bigrams_list_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "401e25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 create a list that only contains bigram, no unigram\n",
    "flat_bi_non = []\n",
    "flat_bi_non = [item for item in flat_uni_bi_non if item not in flat_uni_non]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c131fcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i_am', 72), ('can_be', 52), ('this_is', 39), ('to_qanon', 39), ('and_our', 38), ('share_anything', 37), ('to_follow', 37), ('use_this', 36), ('thread_to', 36), ('interesting_related', 36), ('cause_this', 36), ('pictures_news', 36), ('links_podcasts', 36), ('videos_etc', 36), ('please_remember', 36), ('our_rules', 36), ('and_keep', 36), ('conversations_civil', 36), ('if_you', 34), ('i_was', 34), ('but_i', 31), ('i_have', 27), ('it_was', 27), ('thank_you', 26), ('you_can', 25), ('i_dont', 25), ('i_would', 24), ('out_of', 23), ('we_are', 23), ('want_to', 22), ('i_will', 22), ('would_be', 22), ('they_are', 21), ('people_who', 21), ('im_a', 21), ('some_of', 21), ('i_want', 19), ('to_hear', 19), ('as_a', 19), ('will_be', 18), ('has_been', 18), ('a_lot', 18), ('to_get', 18), ('someone_who', 17), ('going_to', 17), ('trying_to', 16), ('conspiracy_theories', 16), ('have_been', 16), ('to_do', 16), ('believe_in', 15)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Extract top 50 frequent bigram of the non-recovery group\n",
    "from collections import Counter\n",
    "Counter = Counter(flat_bi_non)\n",
    "most_common_bigram_non = Counter.most_common(50)\n",
    "print(most_common_bigram_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a527995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8ef269cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put trigram in a flat list\n",
    "trigrams_list_non = []\n",
    "for i in range(len(text_split_non)):\n",
    "    trigrams_list_non.append(trigram_non[bigram_non[text_split_non[i]]])\n",
    "flat_uni_bi_tri_non = flatten(trigrams_list_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "437fd58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a list that only contains trigrams, no bigram, no unigram\n",
    "flat_tri_non = []\n",
    "flat_tri_temp_non = []\n",
    "flat_tri_temp_non = [item for item in flat_uni_bi_tri_non if item not in flat_bi_non]\n",
    "flat_tri_non = [item for item in flat_tri_temp_non if item not in flat_uni_non]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "44605200",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('use_this_thread_to', 36), ('share_anything_interesting_related', 36), ('to_qanon_and_our', 36), ('cause_this_can_be', 36), ('pictures_news_links_podcasts', 36), ('videos_etc_please_remember', 36), ('to_follow_our_rules', 36), ('and_keep_conversations_civil', 36), ('to_be', 32), ('it_is', 19), ('i_can', 16), ('so_i', 16), ('i_just', 13), ('i_want_to', 12), ('to_speak_to', 11), ('my_name_is', 9), ('a_lot_of', 9), ('some_of_the', 9), ('people_who_are', 8), ('we_can', 8), ('if_you_are', 7), ('to_hear_from', 7), ('feel_free_to', 6), ('email_me_at', 6), ('working_on_a', 6), ('i_would_like', 6), ('i_could', 6), ('the_rabbit_hole', 6), ('if_this_is', 6), ('we_want_to_hear', 6)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Extract top 50 frequent trigram of the recovery group\n",
    "from collections import Counter\n",
    "Counter = Counter(flat_tri_non)\n",
    "most_common_trigram_non = Counter.most_common(50)\n",
    "print(most_common_trigram_non)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091e1ec1",
   "metadata": {},
   "source": [
    "##  Bigram (w/o stop words) - Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "41a5bcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate a list with no punctuation, but have stop words\n",
    "## The reason that keeps stop words for now is that sequence of words matter to express meaning\n",
    "text_split = []\n",
    "for i in range(len(text_list)):\n",
    "    text_split.append(text_list[i].split())\n",
    "# text_split is a list of list of string tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "6f501b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Remove stop words\n",
    "# 2.1 Edit the stop words list\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# Add some stop words \n",
    "add_stop = ['im', 'one', 'also', 'ive', 'etc', 'hi']\n",
    "stop_words_re = stop_words + add_stop\n",
    "# remove punctuations from stop words\n",
    "for i in range(len(stop_words_re)):\n",
    "    stop_words_re[i] = re.sub(pattern_punc, '', stop_words_re[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "6e3cc112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2. remove stop_words_re (list of string) from text_split (a list of list of string)\n",
    "# save a copy of text_split\n",
    "text_split_no = text_split.copy()\n",
    "# removal\n",
    "for i in range(len(text_split_no)):\n",
    "    for word in text_split_no[i]:\n",
    "        if word in stop_words_re:\n",
    "            text_split_no[i].remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "132ccc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a dictionary\n",
    "dct = corpora.Dictionary(text_split_no)\n",
    "corpus = [dct.doc2bow(line) for line in text_split_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "2dbac3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Build the bigram and trigram model\n",
    "bigram_nostop = gensim.models.phrases.Phrases(text_split_no, min_count=3, threshold=10)\n",
    "trigram_nostop = gensim.models.phrases.Phrases(bigram_nostop[text_split_no], threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "8b949b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Generate a flat list of unigram\n",
    "# Define a function to flatten the lists within list\n",
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist] \n",
    "# put unigrams in a flat list \n",
    "flat_uni = flatten(text_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "336621a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 put bigram in a flat list\n",
    "bigrams_list_nostop = []\n",
    "for i in range(len(text_split_no)):\n",
    "    bigrams_list_nostop.append(bigram_nostop[text_split_no[i]])\n",
    "flat_uni_bi = flatten(bigrams_list_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "950bc0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 create a list that only contains bigram, no unigram\n",
    "flat_bi_nostop = []\n",
    "flat_bi_nostop = [item for item in flat_uni_bi if item not in flat_uni]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "bfe84f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a_lot', 18), ('i_dont', 13), ('deep_state', 11), ('conspiracy_theories', 10), ('rabbit_hole', 9), ('people_like', 9), ('i_am', 9), ('but_i', 8), ('my_parents', 8), ('anyone_else', 7), ('feel_like', 7), ('want_to', 7), ('the_same', 7), ('podesta_emails', 6), ('would_be', 6), ('my_mind', 6), ('social_media', 6), ('my_life', 6), ('mainstream_media', 6), ('dont_know', 6), ('q_anon', 5), ('mental_health', 5), ('this_cult', 5), ('make_sense', 5), ('years_old', 5), ('might_be', 5), ('to_be', 5), ('moms_side', 5), ('a_person', 5), ('far_right', 5), ('both_sides', 5), ('even_though', 4), ('conspiracy_theory', 4), ('when_trump', 4), ('many_people', 4), ('never_really', 4), ('qs_claims', 4), ('sealed_indictments', 4), ('campaigns_like', 4), ('everything_connected', 4), ('great_awakening', 4), ('economic_elites', 4), ('internally_consistent', 4), ('q_drops', 4), ('collective_sensemaking', 4), ('they_are', 4), ('it_seems', 4), ('have_lost', 4), ('health_crisis', 2), ('dont_want', 2)]\n"
     ]
    }
   ],
   "source": [
    "# 6. Extract top 50 frequent bigram of the recovery group\n",
    "from collections import Counter\n",
    "Counter = Counter(flat_bi_nostop)\n",
    "most_common_bigram_nostop = Counter.most_common(50)\n",
    "print(most_common_bigram_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "0e1e75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_counts = dict(Counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4efb7",
   "metadata": {},
   "source": [
    "##  Bigram (w/o stop words) - Non-recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "cece827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Load the data of non-recovery group - Extract the rows with is_recovery being NaN\n",
    "df_non = selected_columns.copy()\n",
    "df_non.drop(df_non.index[df_non['is_recovery'] == 1.0], inplace=True)\n",
    "df_non.drop(columns = ['is_recovery'], inplace = True)\n",
    "df_non.dropna(subset = ['text'], inplace = True)\n",
    "df_non.reset_index(drop = True, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "819b5bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 1)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "87d91b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1 Define patterns to remove punctuations, numbers, new lines, multiple spaces\n",
    "pattern_punc = \"[^\\w\\s]\"\n",
    "pattern_num = \"[0-9]\"\n",
    "pattern_newline = \"\\n\"\n",
    "pattern_mulspace = \"\\s+\"\n",
    "pattern_longline = \"___\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "4acffd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2 Remove punctuation patterns\n",
    "for i in range(len(df_non)):\n",
    "        # remove numbers\n",
    "        df_non['text'][i] = re.sub(pattern_num, '', df_non['text'][i])\n",
    "        # lowercase\n",
    "        df_non['text'][i] = df_non['text'][i].lower()\n",
    "        # remove puncuations\n",
    "        df_non['text'][i] = re.sub(pattern_punc, '', df_non['text'][i])\n",
    "        # remove new line (\\n)\n",
    "        df_non['text'][i] = re.sub(pattern_newline, ' ', df_non['text'][i])\n",
    "        # remove multiple space\n",
    "        df_non['text'][i] = re.sub(pattern_mulspace, ' ', df_non['text'][i])  \n",
    "        # remove long line\n",
    "        df_non['text'][i] = re.sub(pattern_longline, ' ', df_non['text'][i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "b15d60fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Tokenization, put all rows into a list\n",
    "text_list_non = df_non['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "a8de0ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1. Generate a list with no punctuation, but have stop words\n",
    "text_split_non = []\n",
    "for i in range(len(text_list_non)):\n",
    "    text_split_non.append(text_list_non[i].split())\n",
    "# text_split_non is a list of list of string tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "d83c7a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2. Remove stop words\n",
    "# 2.2.1 Edit the stop words list\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# Add some stop words \n",
    "add_stop = ['im', 'one', 'also', 'ive', 'etc', 'hi']\n",
    "stop_words_re = stop_words + add_stop\n",
    "# remove punctuations from stop words\n",
    "for i in range(len(stop_words_re)):\n",
    "    stop_words_re[i] = re.sub(pattern_punc, '', stop_words_re[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "0f369fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.2 remove stop_words_re (list of string) from text_split (a list of list of string)\n",
    "# save a copy of text_split\n",
    "non_text_split_no = text_split_non.copy()\n",
    "# removal\n",
    "for i in range(len(non_text_split_no)):\n",
    "    for word in non_text_split_no[i]:\n",
    "        if word in stop_words_re:\n",
    "            non_text_split_no[i].remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "26d21fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_text_split_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "60717f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a dictionary\n",
    "dct_non = corpora.Dictionary(non_text_split_no)\n",
    "corpus_non = [dct.doc2bow(line) for line in non_text_split_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "172c569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Build the bigram and trigram model\n",
    "bigram_non = gensim.models.phrases.Phrases(non_text_split_no, min_count=3, threshold=5)\n",
    "trigram_non = gensim.models.phrases.Phrases(bigram_non[non_text_split_no], threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "2045e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Generate a flat list of unigram\n",
    "# Define a function to flatten the lists within list\n",
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist] \n",
    "# put unigrams in a flat list \n",
    "non_flat_uni = flatten(text_split_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "b555653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 put bigram in a flat list\n",
    "non_bigrams_list = []\n",
    "for i in range(len(non_text_split_no)):\n",
    "    non_bigrams_list.append(bigram_non[non_text_split_no[i]])\n",
    "non_flat_uni_bi = flatten(non_bigrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "0b8d9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 create a list that only contains bigram, no unigram\n",
    "non_flat_bi = []\n",
    "non_flat_bi = [item for item in non_flat_uni_bi if item not in non_flat_uni]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "92fc6dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('share_anything', 37), ('use_thread', 36), ('interesting_related', 36), ('qanon_cause', 36), ('can_pictures', 36), ('news_links', 36), ('podcasts_videos', 36), ('please_remember', 36), ('follow_our', 36), ('rules_keep', 36), ('conversations_civil', 36), ('would_like', 17), ('conspiracy_theories', 16), ('thank_you', 14), ('i_am', 13), ('the_time', 11), ('i_have', 10), ('i_was', 10), ('i_want', 9), ('a_lot', 9), ('inclusion_criteria', 9), ('feel_free', 8), ('i_dont', 8), ('the_theory', 8), ('the_mod', 8), ('want_hear', 7), ('so_much', 7), ('even_though', 7), ('the_world', 7), ('we_will', 7), ('feel_like', 7), ('qanon_conspiracy', 6), ('conspiracy_theory', 6), ('media_requests', 6), ('id_love', 6), ('please_send', 6), ('the_same', 6), ('loved_ones', 6), ('the_rabbit', 6), ('am_looking', 6), ('know_someone', 6), ('max_erdemandi', 6), ('those_have', 5), ('help_people', 5), ('qanon_followers', 5), ('the_new', 5), ('this_sub', 5), ('this_thread', 5), ('please_feel', 5), ('people_are', 5)]\n"
     ]
    }
   ],
   "source": [
    "# 6. Extract top 50 frequent bigram of the non-recovery group\n",
    "from collections import Counter\n",
    "Counter_non = Counter(non_flat_bi)\n",
    "most_common_bigram_non = Counter_non.most_common(50)\n",
    "print(most_common_bigram_non)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c1f739",
   "metadata": {},
   "source": [
    "### Bigrams w/o stop words, all posts (rec & non-rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "3bac847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('recovery_samples_discosure_subs - recovery_samples.csv')\n",
    "all_post = sample[['text']] # include both recovery and non-recovery groups\n",
    "df_all = all_post.copy()\n",
    "df_all.dropna(subset = ['text'], inplace = True) # remove rows with empty text columns\n",
    "df_all.reset_index(drop = True, inplace = True) \n",
    "# reset the index, if not, there will be a gap caused by dropping na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "3818059b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242, 1)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "1cd83098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Define patterns to remove punctuations, numbers, new lines, multiple spaces\n",
    "pattern_punc = \"[^\\w\\s]\"\n",
    "pattern_num = \"[0-9]\"\n",
    "pattern_newline = \"\\n\"\n",
    "pattern_mulspace = \"\\s+\"\n",
    "pattern_longline = \"___\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "9981bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Remove punctuation patterns\n",
    "for i in range(len(df_all)):\n",
    "        # lowercase\n",
    "        df_all['text'][i] = df_all['text'][i].lower()\n",
    "        # remove puncuations\n",
    "        df_all['text'][i] = re.sub(pattern_punc, '', df_all['text'][i])\n",
    "        # remove numbers\n",
    "        df_all['text'][i] = re.sub(pattern_num, '', df_all['text'][i])\n",
    "        # remove new line (\\n)\n",
    "        df_all['text'][i] = re.sub(pattern_newline, ' ', df_all['text'][i])\n",
    "        # remove multiple space\n",
    "        df_all['text'][i] = re.sub(pattern_mulspace, ' ', df_all['text'][i])  \n",
    "        # remove long line\n",
    "        df_all['text'][i] = re.sub(pattern_longline, ' ', df_all['text'][i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "cc4a4801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Tokenization, put all rows into a list\n",
    "all_text_list = df_all['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "547340d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Tokenization using the split function \n",
    "all_word_list = []\n",
    "for i in all_text_list:\n",
    "    if type(i) != str:\n",
    "        continue\n",
    "    else:\n",
    "        words = i.split()\n",
    "        # word_list.append(words)\n",
    "        for word in words:\n",
    "            all_word_list.append(word)\n",
    "# Method 2: nltk.word_tokenize('here is your sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "75fd3e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Edit the stop words list\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# Add some stop words \n",
    "add_stop = ['im', 'one', 'also', 'ive', 'etc', 'hi']\n",
    "stop_words_re = stop_words + add_stop\n",
    "# remove punctuations from stop words\n",
    "for i in range(len(stop_words_re)):\n",
    "    stop_words_re[i] = re.sub(pattern_punc, '', stop_words_re[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "f0d25b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_list_nostop = []\n",
    "# 4.2 Remove stop words\n",
    "for word in all_word_list:\n",
    "    if word not in stop_words_re:\n",
    "        all_word_list_nostop.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "50e12512",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_all = all_word_list_nostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "5cd3cc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('like', 200), ('people', 190), ('qanon', 187), ('q', 183), ('would', 132), ('know', 127), ('even', 101), ('time', 96), ('want', 94), ('conspiracy', 93), ('believe', 91), ('really', 83), ('please', 83), ('get', 82), ('trump', 75), ('think', 74), ('going', 73), ('help', 69), ('feel', 68), ('much', 65), ('news', 65), ('back', 65), ('anything', 64), ('world', 64), ('share', 64), ('still', 63), ('someone', 61), ('things', 60), ('could', 60), ('right', 59), ('say', 58), ('never', 54), ('anyone', 53), ('follow', 53), ('us', 53), ('way', 53), ('videos', 53), ('family', 52), ('got', 51), ('first', 51), ('use', 51), ('remember', 50), ('something', 50), ('lot', 49), ('see', 49), ('go', 48), ('story', 48), ('thread', 48), ('said', 47), ('always', 47)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Extract top 50 frequent unigram of the recovery group\n",
    "from collections import Counter\n",
    "Counter_all = Counter(unigram_all)\n",
    "most_common_unigram_all= Counter_all.most_common(50)\n",
    "print(most_common_unigram_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac30c5",
   "metadata": {},
   "source": [
    "### bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "adadf85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "45b69f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate a list with no punctuation, but have stop words\n",
    "text_split_all = []\n",
    "for i in range(len(all_text_list)):\n",
    "    text_split_all.append(all_text_list[i].split())\n",
    "# text_split is a list of list of string tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "4fbf0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Remove stop words\n",
    "# 2.1 Edit the stop words list\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# Add some stop words \n",
    "add_stop = ['im', 'one', 'also', 'ive', 'etc', 'hi']\n",
    "stop_words_re = stop_words + add_stop\n",
    "# remove punctuations from stop words\n",
    "for i in range(len(stop_words_re)):\n",
    "    stop_words_re[i] = re.sub(pattern_punc, '', stop_words_re[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "1c419649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2. remove stop_words_re (list of string) from text_split (a list of list of string)\n",
    "# save a copy of text_split\n",
    "text_split_no_all = text_split_all.copy()\n",
    "# removal\n",
    "for i in range(len(text_split_no_all)):\n",
    "    for word in text_split_no_all[i]:\n",
    "        if word in stop_words_re:\n",
    "            text_split_no_all[i].remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "f938aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a dictionary\n",
    "dct_all = corpora.Dictionary(text_split_no_all)\n",
    "corpus_all = [dct.doc2bow(line) for line in text_split_no_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "448616d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Build the bigram and trigram model\n",
    "bigram_all = gensim.models.phrases.Phrases(text_split_no_all, min_count=3, threshold=5)\n",
    "trigram_all = gensim.models.phrases.Phrases(bigram_all[text_split_no_all], threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "001658cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Generate a flat list of unigram\n",
    "# Define a function to flatten the lists within list\n",
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist] \n",
    "# put unigrams in a flat list \n",
    "flat_uni_all = flatten(text_split_no_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "66cbd50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 put bigram in a flat list\n",
    "bigrams_list_all = []\n",
    "for i in range(len(text_split_no_all)):\n",
    "    bigrams_list_all.append(bigram_all[text_split_no_all[i]])\n",
    "flat_uni_bi_all = flatten(bigrams_list_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "aacfaae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 create a list that only contains bigram, no unigram\n",
    "flat_bi_all = []\n",
    "flat_bi_all = [item for item in flat_uni_bi_all if item not in flat_uni_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "a9fb4c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('share_anything', 37), ('use_thread', 36), ('interesting_related', 36), ('qanon_cause', 36), ('can_pictures', 36), ('news_links', 36), ('podcasts_videos', 36), ('please_remember', 36), ('follow_our', 36), ('rules_keep', 36), ('conversations_civil', 36), ('a_lot', 27), ('conspiracy_theories', 27), ('i_was', 24), ('the_world', 24), ('i_dont', 21), ('i_am', 21), ('would_like', 18), ('i_have', 16), ('thank_you', 16), ('deep_state', 15), ('i_want', 14), ('the_same', 13), ('but_i', 12), ('the_first', 12), ('even_though', 11), ('conspiracy_theory', 11), ('feel_like', 11), ('rabbit_hole', 11), ('would_be', 11), ('my_parents', 11), ('anyone_else', 10), ('the_whole', 10), ('i_had', 10), ('people_like', 10), ('it_is', 10), ('i_feel', 9), ('it_was', 9), ('i_just', 9), ('my_life', 9), ('this_is', 9), ('people_are', 9), ('mainstream_media', 9), ('the_new', 9), ('the_theory', 9), ('inclusion_criteria', 9), ('i_cant', 8), ('my_mind', 8), ('many_people', 8), ('social_media', 8)]\n"
     ]
    }
   ],
   "source": [
    "# 6. Extract top 50 frequent bigram of the recovery group\n",
    "from collections import Counter\n",
    "Counter_all = Counter(flat_bi_all)\n",
    "most_common_bigram_all = Counter_all.most_common(50)\n",
    "print(most_common_bigram_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "97f9db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_counts = dict(Counter_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d728f17",
   "metadata": {},
   "source": [
    "### Save base_counts to tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "44cf2521",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.DataFrame.from_dict(base_counts, orient='index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "ef300d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = df_base.rename(columns={'index':'bigram', 0:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "f64e74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.to_csv(\"base_counts.tsv\", sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b004caf6",
   "metadata": {},
   "source": [
    "### Save recovery_counts to tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "4bc95688",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovery = pd.DataFrame.from_dict(recovery_counts, orient='index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "2e0a7a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovery = df_recovery.rename(columns={'index':'bigram', 0:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "28961969",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovery.to_csv(\"recovery_counts.tsv\", sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a8a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
