{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d102667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # gensim depends on numpy\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef219a78",
   "metadata": {},
   "source": [
    "# The Recovery Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b95bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract the rows with is_recovery = 1\n",
    "sample = pd.read_csv('recovery_samples_discosure_subs - recovery_samples.csv')\n",
    "selected_columns = sample[['text', 'is_recovery']]\n",
    "df_rec = selected_columns.copy()\n",
    "df_rec.dropna(subset = ['is_recovery'], inplace = True)\n",
    "df_rec.dropna(subset = ['text'], inplace = True)\n",
    "df_rec.drop(columns = ['is_recovery'], inplace = True)\n",
    "df_rec.reset_index(drop = True, inplace = True) \n",
    "# reset the index, if not, there will be a gap caused by dropping na values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0777621d",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d0abb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Define patterns to remove punctuations, numbers, new lines, multiple spaces\n",
    "pattern_punc = \"[^\\w\\s]\"\n",
    "pattern_num = \"[0-9]\"\n",
    "pattern_newline = \"\\n\"\n",
    "pattern_mulspace = \"\\s+\"\n",
    "pattern_longline = \"___\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "366f2f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Remove punctuation patterns\n",
    "for i in range(len(df_rec)):\n",
    "        # lowercase\n",
    "        df_rec['text'][i] = df_rec['text'][i].lower()\n",
    "        # remove puncuations\n",
    "        df_rec['text'][i] = re.sub(pattern_punc, '', df_rec['text'][i])\n",
    "        # remove numbers\n",
    "        df_rec['text'][i] = re.sub(pattern_num, '', df_rec['text'][i])\n",
    "        # remove new line (\\n)\n",
    "        df_rec['text'][i] = re.sub(pattern_newline, ' ', df_rec['text'][i])\n",
    "        # remove multiple space\n",
    "        df_rec['text'][i] = re.sub(pattern_mulspace, ' ', df_rec['text'][i])  \n",
    "        # remove long line\n",
    "        df_rec['text'][i] = re.sub(pattern_longline, ' ', df_rec['text'][i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9b152e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Tokenization, put all rows into a list\n",
    "text_list = df_rec['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ae158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Tokenization using the split function \n",
    "word_list = []\n",
    "for i in text_list:\n",
    "    if type(i) != str:\n",
    "        continue\n",
    "    else:\n",
    "        words = i.split()\n",
    "        # word_list.append(words)\n",
    "        for word in words:\n",
    "            word_list.append(word)\n",
    "# Method 2: nltk.word_tokenize('here is your sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f12ed9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19760"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e0036e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Edit the stop words list\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# Add some stop words \n",
    "add_stop = ['im', 'one', 'also', 'ive', 'etc', 'hi']\n",
    "stop_words_re = stop_words + add_stop\n",
    "# remove punctuations from stop words\n",
    "for i in range(len(stop_words_re)):\n",
    "    stop_words_re[i] = re.sub(pattern_punc, '', stop_words_re[i])\n",
    "    word_list_nostop = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa212a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Remove stop words\n",
    "for word in word_list:\n",
    "    if word not in stop_words_re:\n",
    "        word_list_nostop.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12c889e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = word_list_nostop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ec09246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('q', 95), ('like', 93), ('people', 83), ('believe', 53), ('would', 46), ('know', 45), ('even', 45), ('time', 44), ('really', 43), ('conspiracy', 43), ('trump', 43), ('world', 42), ('think', 41), ('going', 41), ('qanon', 38), ('right', 35), ('much', 35), ('things', 34), ('want', 33), ('never', 32), ('back', 32), ('lot', 29), ('could', 29), ('way', 27), ('still', 27), ('started', 27), ('covid', 27), ('get', 26), ('everything', 26), ('help', 26), ('say', 25), ('feel', 25), ('true', 24), ('something', 24), ('reality', 24), ('us', 23), ('got', 23), ('family', 23), ('new', 22), ('stuff', 22), ('well', 22), ('government', 22), ('actually', 21), ('made', 21), ('believed', 21), ('story', 21), ('far', 21), ('thing', 20), ('qs', 20), ('years', 20)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Extract top 50 frequent unigram of the recovery group\n",
    "from collections import Counter\n",
    "Counter = Counter(unigram)\n",
    "most_common_unigram= Counter.most_common(50)\n",
    "print(most_common_unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7fee49",
   "metadata": {},
   "source": [
    "##  Bigram & Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b769b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58344ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate a list with no punctuation, but have stop words\n",
    "## The reason that keeps stop words for now is that sequence of words matter to express meaning\n",
    "text_split = []\n",
    "for i in range(len(text_list)):\n",
    "    text_split.append(text_list[i].split())\n",
    "# text_split is a list of list of string tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56f4327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a dictionary\n",
    "dct = corpora.Dictionary(text_split)\n",
    "corpus = [dct.doc2bow(line) for line in text_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a212543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build the bigram and trigram model\n",
    "bigram = gensim.models.phrases.Phrases(text_split, min_count=3, threshold=5)\n",
    "trigram = gensim.models.phrases.Phrases(bigram[text_split], threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60458b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how the bigram looks \n",
    "# print(bigram[text_split[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7e45ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Generate a flat list of unigram\n",
    "# Define a function to flatten the lists within list\n",
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist] \n",
    "# put unigrams in a flat list \n",
    "flat_uni = flatten(text_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6813c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 put bigram in a flat list\n",
    "bigrams_list = []\n",
    "for i in range(len(text_split)):\n",
    "    bigrams_list.append(bigram[text_split[i]])\n",
    "flat_uni_bi = flatten(bigrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6493a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 create a list that only contains bigram, no unigram\n",
    "flat_bi = []\n",
    "flat_bi = [item for item in flat_uni_bi if item not in flat_uni]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30d3a318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i_was', 64), ('but_i', 43), ('to_be', 37), ('i_had', 31), ('a_lot', 29), ('at_the', 29), ('i_dont', 29), ('it_was', 26), ('i_am', 26), ('the_world', 24), ('as_a', 22), ('going_to', 21), ('it_is', 21), ('want_to', 20), ('out_of', 20), ('i_just', 17), ('i_think', 17), ('have_been', 16), ('some_of', 15), ('the_whole', 14), ('back_to', 14), ('a_few', 14), ('im_not', 13), ('the_most', 13), ('to_say', 13), ('i_do', 13), ('when_i', 12), ('about_it', 12), ('i_feel', 11), ('as_well', 11), ('the_government', 11), ('they_are', 11), ('ive_been', 11), ('there_are', 11), ('for_me', 10), ('my_story', 10), ('the_deep', 10), ('could_be', 10), ('who_are', 10), ('of_these', 10), ('my_life', 10), ('the_same', 10), ('thank_you', 10), ('kind_of', 10), ('like_this', 9), ('people_who', 9), ('rabbit_hole', 9), ('if_you', 9), ('wanted_to', 9), ('in_order', 9)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Extract top 50 frequent bigram of the recovery group\n",
    "from collections import Counter\n",
    "Counter = Counter(flat_bi)\n",
    "most_common_bigram = Counter.most_common(50)\n",
    "print(most_common_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6209b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c19c71fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put trigram in a flat list\n",
    "trigrams_list = []\n",
    "for i in range(len(text_split)):\n",
    "    trigrams_list.append(trigram[bigram[text_split[i]]])\n",
    "flat_uni_bi_tri = flatten(trigrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea9c27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a list that only contains trigrams, no bigram, no unigram\n",
    "flat_tri = []\n",
    "flat_tri_temp = []\n",
    "flat_tri_temp = [item for item in flat_uni_bi_tri if item not in flat_bi]\n",
    "flat_tri = [item for item in flat_tri_temp if item not in flat_uni]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04d59ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flat_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f54bb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a_lot_of', 21), ('i_would', 10), ('to_do', 10), ('the_deep_state', 9), ('i_never', 8), ('i_know', 8), ('in_order_to', 7), ('at_the_time', 7), ('as_much_as', 6), ('a_part_of', 6)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Extract top 50 frequent trigram of the recovery group\n",
    "from collections import Counter\n",
    "Counter = Counter(flat_tri)\n",
    "most_common_trigram = Counter.most_common(50)\n",
    "print(most_common_trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b4a0c",
   "metadata": {},
   "source": [
    "# The Non-recovery Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "291c31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract the rows with is_recovery being NaN\n",
    "df_non = selected_columns.copy()\n",
    "df_non.drop(df_non.index[df_non['is_recovery'] == 1.0], inplace=True)\n",
    "\n",
    "df_non.drop(columns = ['is_recovery'], inplace = True)\n",
    "df_non.dropna(subset = ['text'], inplace = True)\n",
    "\n",
    "df_non.reset_index(drop = True, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "009cc301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb2a9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Define patterns to remove punctuations, numbers, new lines, multiple spaces\n",
    "pattern_punc = \"[^\\w\\s]\"\n",
    "pattern_num = \"[0-9]\"\n",
    "pattern_newline = \"\\n\"\n",
    "pattern_mulspace = \"\\s+\"\n",
    "pattern_longline = \"___\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9d6e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Remove punctuation patterns\n",
    "for i in range(len(df_non)):\n",
    "        # remove numbers\n",
    "        df_non['text'][i] = re.sub(pattern_num, '', df_non['text'][i])\n",
    "        # lowercase\n",
    "        df_non['text'][i] = df_non['text'][i].lower()\n",
    "        # remove puncuations\n",
    "        df_non['text'][i] = re.sub(pattern_punc, '', df_non['text'][i])\n",
    "        # remove new line (\\n)\n",
    "        df_non['text'][i] = re.sub(pattern_newline, ' ', df_non['text'][i])\n",
    "        # remove multiple space\n",
    "        df_non['text'][i] = re.sub(pattern_mulspace, ' ', df_non['text'][i])  \n",
    "        # remove long line\n",
    "        df_non['text'][i] = re.sub(pattern_longline, ' ', df_non['text'][i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcb1f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Tokenization, put all rows into a list\n",
    "text_list_non = df_non['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db6856db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Tokenization using the split function \n",
    "word_list_non = []\n",
    "for i in text_list_non:\n",
    "    if type(i) != str:\n",
    "        continue\n",
    "    else:\n",
    "        words = i.split()\n",
    "        # word_list.append(words)\n",
    "        for word in words:\n",
    "            word_list_non.append(word)\n",
    "# Method 2: nltk.word_tokenize('here is your sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13507e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25223"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33ab18d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Remove stop words\n",
    "word_list_non_nostop = []\n",
    "for word in word_list_non:\n",
    "    if word not in stop_words_re:\n",
    "        word_list_non_nostop.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b829ad8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12138"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list_non_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "beaca769",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_non = word_list_non_nostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f24ceaf1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('qanon', 149), ('people', 107), ('like', 107), ('q', 88), ('would', 86), ('know', 82), ('please', 77), ('want', 61), ('share', 59), ('get', 56), ('even', 56), ('anything', 55), ('time', 52), ('conspiracy', 50), ('someone', 47), ('use', 46), ('news', 46), ('follow', 46), ('thread', 45), ('cause', 44), ('feel', 43), ('help', 43), ('videos', 43), ('remember', 43), ('keep', 41), ('really', 40), ('interesting', 40), ('related', 40), ('believe', 38), ('pictures', 38), ('links', 38), ('rules', 38), ('conversations', 38), ('civil', 38), ('anyone', 37), ('still', 36), ('podcasts', 36), ('research', 34), ('make', 33), ('first', 33), ('say', 33), ('back', 33), ('think', 33), ('trump', 32), ('going', 32), ('always', 31), ('could', 31), ('theories', 30), ('much', 30), ('see', 30)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Extract top 50 frequent unigram of the non-recovery group\n",
    "from collections import Counter\n",
    "Counter = Counter(unigram_non)\n",
    "most_common_unigram_non= Counter.most_common(50)\n",
    "print(most_common_unigram_non)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02697ed",
   "metadata": {},
   "source": [
    "## Bigram & Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b84a695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_list_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1339821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate a list with no punctuation, but have stop words\n",
    "## The reason that keeps stop words for now is that sequence of words matter to express meaning\n",
    "text_split_non = []\n",
    "for i in range(len(text_list_non)):\n",
    "    text_split_non.append(text_list_non[i].split())\n",
    "# text_split_non is a list of list of string tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8041f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a dictionary\n",
    "dct_non = corpora.Dictionary(text_split_non)\n",
    "corpus_non = [dct_non.doc2bow(line) for line in text_split_non]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a5bef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build the bigram and trigram model\n",
    "bigram_non = gensim.models.phrases.Phrases(text_split_non, min_count=3, threshold=5)\n",
    "trigram_non = gensim.models.phrases.Phrases(bigram_non[text_split_non], threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "daae0e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i_have', 'created', 'this', 'subreddit', 'for_those', 'who_have', 'escaped', 'qanon', 'feel_free', 'to_share', 'your', 'stories', 'and', 'your', 'struggles', 'how_did', 'you', 'get', 'into_q', 'how_did', 'you', 'get', 'out', 'this_is', 'the', 'sistersubreddit', 'of', 'rqanoncasualties', 'i_hope', 'this', 'community', 'will_be', 'as', 'helpful', 'to', 'you', 'as', 'that', 'community', 'has_been', 'for_those', 'who_have', 'lost', 'family', 'and', 'friends', 'to', 'q']\n"
     ]
    }
   ],
   "source": [
    "# see how the bigram looks \n",
    "print(bigram_non[text_split_non[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45b4d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Generate a flat list of unigram\n",
    "# put unigrams in a flat list \n",
    "flat_uni_non = flatten(text_split_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43d8993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 put bigram in a flat list\n",
    "bigrams_list_non = []\n",
    "for i in range(len(text_split_non)):\n",
    "    bigrams_list_non.append(bigram_non[text_split_non[i]])\n",
    "flat_uni_bi_non = flatten(bigrams_list_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "401e25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 create a list that only contains bigram, no unigram\n",
    "flat_bi_non = []\n",
    "flat_bi_non = [item for item in flat_uni_bi_non if item not in flat_uni_non]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c131fcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i_am', 72), ('can_be', 52), ('this_is', 39), ('to_qanon', 39), ('and_our', 38), ('share_anything', 37), ('to_follow', 37), ('use_this', 36), ('thread_to', 36), ('interesting_related', 36), ('cause_this', 36), ('pictures_news', 36), ('links_podcasts', 36), ('videos_etc', 36), ('please_remember', 36), ('our_rules', 36), ('and_keep', 36), ('conversations_civil', 36), ('if_you', 34), ('i_was', 34), ('but_i', 31), ('i_have', 27), ('it_was', 27), ('thank_you', 26), ('you_can', 25), ('i_dont', 25), ('i_would', 24), ('out_of', 23), ('we_are', 23), ('want_to', 22), ('i_will', 22), ('would_be', 22), ('they_are', 21), ('people_who', 21), ('im_a', 21), ('some_of', 21), ('i_want', 19), ('to_hear', 19), ('as_a', 19), ('will_be', 18), ('has_been', 18), ('a_lot', 18), ('to_get', 18), ('someone_who', 17), ('going_to', 17), ('trying_to', 16), ('conspiracy_theories', 16), ('have_been', 16), ('to_do', 16), ('believe_in', 15)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Extract top 50 frequent bigram of the recovery group\n",
    "from collections import Counter\n",
    "Counter = Counter(flat_bi_non)\n",
    "most_common_bigram_non = Counter.most_common(50)\n",
    "print(most_common_bigram_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a527995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ef269cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put trigram in a flat list\n",
    "trigrams_list_non = []\n",
    "for i in range(len(text_split_non)):\n",
    "    trigrams_list_non.append(trigram_non[bigram_non[text_split_non[i]]])\n",
    "flat_uni_bi_tri_non = flatten(trigrams_list_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "437fd58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a list that only contains trigrams, no bigram, no unigram\n",
    "flat_tri_non = []\n",
    "flat_tri_temp_non = []\n",
    "flat_tri_temp_non = [item for item in flat_uni_bi_tri_non if item not in flat_bi_non]\n",
    "flat_tri_non = [item for item in flat_tri_temp_non if item not in flat_uni_non]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44605200",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('use_this_thread_to', 36), ('share_anything_interesting_related', 36), ('to_qanon_and_our', 36), ('cause_this_can_be', 36), ('pictures_news_links_podcasts', 36), ('videos_etc_please_remember', 36), ('to_follow_our_rules', 36), ('and_keep_conversations_civil', 36), ('to_be', 32), ('it_is', 19), ('i_can', 16), ('so_i', 16), ('i_just', 13), ('i_want_to', 12), ('to_speak_to', 11), ('my_name_is', 9), ('a_lot_of', 9), ('some_of_the', 9), ('people_who_are', 8), ('we_can', 8), ('if_you_are', 7), ('to_hear_from', 7), ('feel_free_to', 6), ('email_me_at', 6), ('working_on_a', 6), ('i_would_like', 6), ('i_could', 6), ('the_rabbit_hole', 6), ('if_this_is', 6), ('we_want_to_hear', 6)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Extract top 50 frequent trigram of the recovery group\n",
    "from collections import Counter\n",
    "Counter = Counter(flat_tri_non)\n",
    "most_common_trigram_non = Counter.most_common(50)\n",
    "print(most_common_trigram_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea691ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac59080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66655da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b1ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f945302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79874168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
